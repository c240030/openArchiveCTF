{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3fc891f8",
   "metadata": {},
   "source": [
    "# MLMpire CTF: Guided Flag Extraction\n",
    "This notebook follows the intended path: avoid the 'api/auth...' decoy by choosing 'A' after 'password: ', then steer to 'AI2025{' and constrain characters inside braces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd3c0de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies (Windows/Colab-safe)\n",
    "import sys\n",
    "!{sys.executable} -m pip install --quiet transformers torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b2a66f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from architecture import load_hf_gpt2_model, fill_mask\n",
    "\n",
    "def top_k(wrapper, prompt_text: str, k: int = 10):\n",
    "    logits = fill_mask(wrapper, prompt_text)\n",
    "    probs = F.softmax(logits, dim=-1)\n",
    "    top_k_probs, top_k_indices = torch.topk(probs, k)\n",
    "    results = []\n",
    "    for i in range(k):\n",
    "        idx = top_k_indices[i].item()\n",
    "        results.append((wrapper.itos[idx], top_k_probs[i].item(), idx))\n",
    "    return results\n",
    "\n",
    "def next_char_constrained(wrapper, prompt_text: str, allowed: set[str], k: int = 15):\n",
    "    logits = fill_mask(wrapper, prompt_text)\n",
    "    probs = F.softmax(logits, dim=-1)\n",
    "    top_k_probs, top_k_indices = torch.topk(probs, k)\n",
    "    for i in range(k):\n",
    "        idx = int(top_k_indices[i].item())\n",
    "        tok = wrapper.itos[idx]\n",
    "        if tok in allowed:\n",
    "            return tok, float(top_k_probs[i].item())\n",
    "    idx = int(torch.argmax(probs).item())\n",
    "    return wrapper.itos[idx], float(probs[idx].item())\n",
    "\n",
    "def ensure_in_window(s: str, max_len: int = 120) -> str:\n",
    "    return s[-max_len:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf926bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load wrapper (uses local 'hf_gpt2_model' and 'vocab.json')\n",
    "wrapper = load_hf_gpt2_model()\n",
    "print('Model loaded; vocab size =', len(wrapper.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b4a7df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the initial branch after 'password: ' to see the decoy vs correct path\n",
    "first_top = top_k(wrapper, 'password: [MASK]', k=10)\n",
    "for i, (tok, p, _) in enumerate(first_top, 1):\n",
    "    print(f\"{i:>2}. {tok!r}  p={p:.4f}\")\n",
    "print(\"Hint: choose 'A' here to avoid the log-decoy path.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a55c8ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guided extraction: force 'AI2025{' then constrain inside braces until '}'\n",
    "known = 'password: A'\n",
    "for ch in 'I2025{':\n",
    "    prompt = known[-120:] + '[MASK]'\n",
    "    # Force the intended next character; we print its probability for sanity\n",
    "    logits = fill_mask(wrapper, prompt)\n",
    "    probs = F.softmax(logits, dim=-1)\n",
    "    idxs = [i for i, s in wrapper.itos.items() if s == ch]\n",
    "    p = float(probs[idxs[0]].item()) if idxs else 0.0\n",
    "    print(f'Steer to {ch!r}: p={p:.5f}')\n",
    "    known += ch\n",
    "\n",
    "allowed_inside = set(list('abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789_'))\n",
    "allowed_all = allowed_inside | {'}'}\n",
    "\n",
    "for step in range(256):\n",
    "    if '}' in known:\n",
    "        break\n",
    "    prompt = known[-120:] + '[MASK]'\n",
    "    ch, p = next_char_constrained(wrapper, prompt, allowed_all, k=30)\n",
    "    print(f'next={ch!r} p={p:.5f}')\n",
    "    known += ch\n",
    "\n",
    "print('\n",
    "Known string:', known)\n",
    "s = known.find('AI2025{')\n",
    "e = known.find('}', s+1) if s != -1 else -1\n",
    "flag = known[s:e+1] if (s != -1 and e != -1) else ''\n",
    "print('FLAG:', flag or '(not recovered)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea63c3c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beam search constrained inside braces starting from the known prefix\n",
    "import math\n",
    "\n",
    "# Force the intended starting point as per author hint\n",
    "dec_ctx = 'password: AI2025{'\n",
    "\n",
    "beam = [(0.0, dec_ctx)]  # (logprob, text)\n",
    "beam_width = 5\n",
    "max_steps = 128\n",
    "allowed_inside = set(list('abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789_'))\n",
    "allowed_all = allowed_inside | {'}'}\n",
    "\n",
    "for step in range(max_steps):\n",
    "    new_beam = []\n",
    "    closed = []\n",
    "    for score, cand in beam:\n",
    "        if '}' in cand:\n",
    "            closed.append((score, cand))\n",
    "            continue\n",
    "        prompt = cand[-120:] + '[MASK]'\n",
    "        logits = fill_mask(wrapper, prompt)\n",
    "        logp = F.log_softmax(logits, dim=-1)\n",
    "        top_logp, top_idx = torch.topk(logp, 80)\n",
    "        for i in range(top_logp.numel()):\n",
    "            idx = int(top_idx[i].item())\n",
    "            tok = wrapper.itos[idx]\n",
    "            if tok not in allowed_all:\n",
    "                continue\n",
    "            bonus = 2.0 if tok == '}' else 0.0\n",
    "            new_beam.append((score + float(top_logp[i].item()) + bonus, cand + tok))\n",
    "    if closed:\n",
    "        closed.sort(key=lambda x: x[0], reverse=True)\n",
    "        best = closed[0][1]\n",
    "        print('Recovered from guided beam:', best)\n",
    "        s = best.find('AI2025{')\n",
    "        e = best.find('}', s+1) if s != -1 else -1\n",
    "        flag = best[s:e+1] if (s != -1 and e != -1) else ''\n",
    "        print('FLAG:', flag or '(not recovered)')\n",
    "        break\n",
    "    if not new_beam:\n",
    "        print('Beam exhausted without closing brace.')\n",
    "        break\n",
    "    new_beam.sort(key=lambda x: x[0], reverse=True)\n",
    "    beam = new_beam[:beam_width]\n",
    "    print(f\"Step {step+1}, best: {beam[0][1][-80:]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d3c84a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional pivot: continue decoding after the decoy tail to redirect toward the flag\n",
    "# If your best path ends with: body={\"name\":\"admin\"}\n",
    "# we try to overwrite that trajectory by forcing the correct prefix next.\n",
    "\n",
    "decoy_tail = 'body={\"name\":\"admin\"}'\n",
    "start = decoy_tail + ' '  # add a space separator if needed\n",
    "\n",
    "# if you captured a longer context in a previous search, paste it into 'start' before running\n",
    "\n",
    "# Force the correct prefix after the decoy\n",
    "forced = start + 'AI2025{'\n",
    "print('Starting from decoy tail, forcing:', forced)\n",
    "\n",
    "beam = [(0.0, forced)]\n",
    "beam_width = 5\n",
    "max_steps = 128\n",
    "allowed_inside = set(list('abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789_'))\n",
    "allowed_all = allowed_inside | {'}'}\n",
    "\n",
    "for step in range(max_steps):\n",
    "    new_beam = []\n",
    "    closed = []\n",
    "    for score, cand in beam:\n",
    "        if '}' in cand:\n",
    "            closed.append((score, cand))\n",
    "            continue\n",
    "        prompt = cand[-120:] + '[MASK]'\n",
    "        logits = fill_mask(wrapper, prompt)\n",
    "        logp = F.log_softmax(logits, dim=-1)\n",
    "        top_logp, top_idx = torch.topk(logp, 80)\n",
    "        for i in range(top_logp.numel()):\n",
    "            idx = int(top_idx[i].item())\n",
    "            tok = wrapper.itos[idx]\n",
    "            if tok not in allowed_all:\n",
    "                continue\n",
    "            bonus = 2.0 if tok == '}' else 0.0\n",
    "            new_beam.append((score + float(top_logp[i].item()) + bonus, cand + tok))\n",
    "    if closed:\n",
    "        closed.sort(key=lambda x: x[0], reverse=True)\n",
    "        best = closed[0][1]\n",
    "        print('Recovered from decoy pivot:', best)\n",
    "        s = best.find('AI2025{')\n",
    "        e = best.find('}', s+1) if s != -1 else -1\n",
    "        flag = best[s:e+1] if (s != -1 and e != -1) else ''\n",
    "        print('FLAG:', flag or '(not recovered)')\n",
    "        break\n",
    "    if not new_beam:\n",
    "        print('Beam exhausted without closing brace (pivot run).')\n",
    "        break\n",
    "    new_beam.sort(key=lambda x: x[0], reverse=True)\n",
    "    beam = new_beam[:beam_width]\n",
    "    print(f\"[Pivot] Step {step+1}, best: {beam[0][1][-80:]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
